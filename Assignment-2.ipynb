{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DOWNLOADING TEXTBOOK FROM PROJECT GUTENBERG \"\"\"\n",
    "\n",
    "import requests\n",
    "response = requests.get(\"http://www.gutenberg.org/files/11/11-0.txt\")\n",
    "text_data = response.text\n",
    "# print(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sentences from the training set - \n",
      "\n",
      "['<s>', 'And', 'now', 'which', 'is', 'which', 'she', 'said', 'to', 'herself', 'and', 'nibbled', 'a', 'little', 'of', 'the', 'righthand', 'bit', 'to', 'try', 'the', 'effect', 'the', 'next', 'moment', 'she', 'felt', 'a', 'violent', 'blow', 'underneath', 'her', 'chin', 'it', 'had', 'struck', 'her', 'foot', '</s>']\n",
      "\n",
      "\n",
      "['<s>', 'And', 'just', 'as', 'Id', 'taken', 'the', 'highest', 'tree', 'in', 'the', 'wood', 'continued', 'the', 'Pigeon', 'raising', 'its', 'voice', 'to', 'a', 'shriek', 'and', 'just', 'as', 'I', 'was', 'thinking', 'I', 'should', 'be', 'free', 'of', 'them', 'at', 'last', 'they', 'must', 'needs', 'come', 'wriggling', 'down', 'from', 'the', 'sky', '</s>']\n",
      "\n",
      "\n",
      "['<s>', 'There', 'was', 'nothing', 'so', 'VERY', 'remarkable', 'in', 'that', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'VERY', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'Rabbit', 'say', 'to', 'itself', 'Oh', 'dear', '</s>']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" PARSING THE DATA USING SENTENCE TOKENIZER \"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences_list = sent_tokenize(text_data)\n",
    "# print(\"\\n\".join(sentences_list))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" REMOVING LINE-BREAKS, CARRIAGE-RETURNS AND NON-ALPHABETICAL CHARACTERS \"\"\"\n",
    "import re\n",
    "def remove(string):\n",
    "    string = string.replace('\\n', ' ')\\\n",
    "                   .replace('\\r', '')\n",
    "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "    string = regex.sub('', string)\n",
    "    return string\n",
    "sentences_list = list(map(remove, sentences_list))\n",
    "\n",
    "\n",
    "\"\"\" SPLITTING DATA INTO TRAINING AND TEST SET \"\"\"\n",
    "import random\n",
    "random.shuffle(sentences_list)\n",
    "sentences_list_train = sentences_list[:int(0.8 * len(sentences_list))]\n",
    "sentences_list_test = sentences_list[int(0.8 * len(sentences_list)):]\n",
    "\n",
    "\n",
    "\"\"\" CREATING VOCBULARY DICTIONARY \"\"\"\n",
    "countTotal = 0\n",
    "dictVocab = dict()\n",
    "for sentence in sentences_list_train:    \n",
    "    for word in sentence.split():\n",
    "        countTotal += 1\n",
    "        if(word not in dictVocab):\n",
    "            dictVocab[word] = 0\n",
    "        dictVocab[word] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\" ADDING <s> and </s> to the start and end of each sentence [TRAINING SET] \"\"\"\n",
    "for s_idx in range(len(sentences_list_train)):\n",
    "    sentences_list_train[s_idx] = \"<s> \" + sentences_list_train[s_idx] + \" </s>\"    \n",
    "    \n",
    "\"\"\" ADDING <s> and </s> to the start and end of each sentence [TEST SET] \"\"\"\n",
    "for s_idx in range(len(sentences_list_test)):\n",
    "    sentences_list_test[s_idx] = \"<s> \" + sentences_list_test[s_idx] + \" </s>\"\n",
    "    \n",
    "    \n",
    "\"\"\" OUTPUT \"\"\"\n",
    "print(\"Some sentences from the training set - \\n\")\n",
    "for s in sentences_list_train[:3]:\n",
    "    print(s.split())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estimation of some bigrams - \n",
      "\n",
      "('<s>', 'And') 0.02631578947368421\n",
      "\n",
      "\n",
      "('And', 'now') 0.017857142857142856\n",
      "\n",
      "\n",
      "('now', 'which') 0.029411764705882353\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "    \n",
    "def gen_ngrams(n):\n",
    "    ngrams_list = []\n",
    "    for sentence in sentences_list_train:\n",
    "        ngrams_list = ngrams_list + list(ngrams(sentence.split(\" \"), n))\n",
    "    return ngrams_list\n",
    "\n",
    "unigrams  = gen_ngrams(1)\n",
    "bigrams   = gen_ngrams(2)\n",
    "trigrams  = gen_ngrams(3)\n",
    "quadgrams = gen_ngrams(4)\n",
    "\n",
    "\n",
    "\"\"\" MLE for unigrams \"\"\"\n",
    "unigramCount = dict()\n",
    "unigramMLE = dict()\n",
    "for key in unigrams:\n",
    "    if(key not in unigramCount):\n",
    "        unigramCount[key] = 0\n",
    "    unigramCount[key] += 1\n",
    "for key in unigramCount:\n",
    "    unigramMLE[key] = unigramCount[key]/len(unigrams)\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" MLE for bigrams \"\"\"\n",
    "bigramCount = dict()\n",
    "bigramMLE = dict()\n",
    "for key in bigrams:\n",
    "    if(key not in bigramCount):\n",
    "        bigramCount[key] = 0\n",
    "    bigramCount[key] += 1\n",
    "for key in bigramCount:\n",
    "    bigramMLE[key] = bigramCount[key]/unigramCount[(key[0], )]\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" MLE for trigrams \"\"\"\n",
    "trigramCount = dict()\n",
    "trigramMLE = dict()\n",
    "for key in trigrams:\n",
    "    if(key not in trigramCount):\n",
    "        trigramCount[key] = 0\n",
    "    trigramCount[key] += 1\n",
    "for key in trigramCount:\n",
    "    trigramMLE[key] = trigramCount[key]/bigramCount[(key[0], key[1])]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" MLE for quadgrams \"\"\"\n",
    "quadgramCount = dict()\n",
    "quadgramMLE = dict()\n",
    "for key in quadgrams:\n",
    "    if(key not in quadgramCount):\n",
    "        quadgramCount[key] = 0\n",
    "    quadgramCount[key] += 1\n",
    "for key in quadgramCount:\n",
    "    quadgramMLE[key] = quadgramCount[key]/trigramCount[(key[0], key[1], key[2])]\n",
    "    \n",
    "    \n",
    "print(\"MLE estimation of some bigrams - \\n\")\n",
    "for key in list(bigramMLE.keys())[:3]:\n",
    "    print(key, bigramMLE[key])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence generated using quadgram model - \n",
      "['<s>', 'Very', 'much', 'indeed', 'and', 'much', 'sooner', 'than', 'she', 'had', 'said', 'that', 'day', 'about', 'it', 'and', 'to', 'hear', 'his', 'shrill', 'little', 'voice', 'alongCatch', 'him', 'you', 'charge', 'for', 'the', 'Rabbits', 'little', 'white', 'one', 'in', 'by', 'way', 'of', 'keeping']\n",
      "\n",
      "\n",
      "Probability [In log space] of the sentence - \"I must have been changed for Mabel\"\n",
      "-27.303160370643322\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy\n",
    "class System:\n",
    "    def generate_sentence(self, model_name):\n",
    "        name2mle = {\"unigram\": unigramMLE, \"bigram\": bigramMLE, \"trigram\": trigramMLE, \"quadgram\": quadgramMLE}\n",
    "        MLE = name2mle[model_name]\n",
    "        sentence = []\n",
    "    \n",
    "        # GENERATE FIRST RANDOM SENTENCE\n",
    "        start = \"<s>\"\n",
    "        key_list = []\n",
    "        value_list = []\n",
    "        for key in MLE:\n",
    "            if(key[0] == start):\n",
    "                key_list.append(key)\n",
    "                value_list.append(MLE[key])\n",
    "                \n",
    "        # NORMALIZE\n",
    "        sum_value_list = sum(value_list)\n",
    "        for zz in range(len(value_list)):\n",
    "            value_list[zz]/=sum_value_list\n",
    "            \n",
    "        randomResult = numpy.random.multinomial(100, value_list, 1).tolist()\n",
    "        sampleIndex = randomResult[0].index(max(randomResult[0]))\n",
    "        sentence += list(key_list[sampleIndex])\n",
    "        \n",
    "        iterations = 0\n",
    "        while(sentence[-1] != \"</s>\"):\n",
    "            start = sentence[-1]\n",
    "            key_list = []\n",
    "            value_list = []\n",
    "            for key in MLE:\n",
    "                if(key[0] == start):\n",
    "                    key_list.append(key)\n",
    "                    value_list.append(MLE[key])\n",
    "            \n",
    "            sum_value_list = sum(value_list)\n",
    "            for zz in range(len(value_list)):\n",
    "                value_list[zz]/=sum_value_list\n",
    "            \n",
    "            randomResult = numpy.random.multinomial(100, value_list, 1).tolist()\n",
    "            sampleIndex = randomResult[0].index(max(randomResult[0]))\n",
    "            sentence += list(key_list[sampleIndex])[1:]\n",
    "            iterations += 1\n",
    "            if(iterations > 10):\n",
    "                break\n",
    "        \n",
    "        return sentence\n",
    "                \n",
    "    def get_probability(self, sentence, model_name):\n",
    "        num2mle  = {1: unigramMLE, 2: bigramMLE, 3: trigramMLE, 4: quadgramMLE}        \n",
    "        name2num = {\"unigram\": 0, \"bigram\": 1, \"trigram\": 2, \"quadgram\": 3}\n",
    "        N = name2num[model_name]\n",
    "        logProb = 1.0\n",
    "        sentence_as_list = sentence.split()\n",
    "        \n",
    "        for idx in range(1, len(sentence_as_list)):\n",
    "            curr_tup = []\n",
    "            for j in range(N, -1, -1):\n",
    "                if(idx-j >= 0):\n",
    "                    curr_tup.append(sentence_as_list[idx-j])\n",
    "            curr_tup = tuple(curr_tup) \n",
    "            tempProb = num2mle[len(curr_tup)][curr_tup]\n",
    "            logProb += math.log2(tempProb)\n",
    "        \n",
    "        return logProb\n",
    "            \n",
    "system = System()\n",
    "model_name_1 = \"quadgram\"\n",
    "print(\"Sample sentence generated using %s model - \" % model_name_1)\n",
    "print(system.generate_sentence(model_name_1))\n",
    "print(\"\\n\")\n",
    "model_name_2 = \"bigram\"\n",
    "print('Probability [In log space] of the sentence - \"I must have been changed for Mabel\"')\n",
    "print(system.get_probability(\"<s> I must have been changed for Mabel </s>\", model_name_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "('<s>', 'And')\n",
      "MLE before Add-1 smoothing - 0.026316\n",
      "After Add-1 smoothing - 0.000903\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "('And', 'now')\n",
      "MLE before Add-1 smoothing - 0.017857\n",
      "After Add-1 smoothing - 0.000078\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "('now', 'which')\n",
      "MLE before Add-1 smoothing - 0.029412\n",
      "After Add-1 smoothing - 0.000078\n",
      "-----------------------------------------\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ADD-1 SMOOTHING \"\"\"\n",
    "\n",
    "vocabSize = len(unigrams)\n",
    "bigramAddOneMLE = dict()\n",
    "for key in bigramCount:\n",
    "    bigramAddOneMLE[key] = (bigramCount[key] + 1)/(unigramCount[(key[0], )] + vocabSize)\n",
    "    \n",
    "for key in list(bigramMLE.keys())[:3]:\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(key)\n",
    "    print(\"MLE before Add-1 smoothing - %f\" % bigramMLE[key])\n",
    "    print(\"After Add-1 smoothing - %f\" % bigramAddOneMLE[key])\n",
    "    print(\"-----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A drastic change in count occurs for the bigram - ('<s\\>', 'And'). This reduced count will be used to estimate the count of bigrams that we have never seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.448150833937636 -0.448150833937636\n",
      "1 0.2948579647608774 0.7051420352391227\n",
      "2 1.0426829268292683 0.9573170731707317\n",
      "3 1.7894736842105263 1.2105263157894737\n",
      "4 3.392156862745098 0.607843137254902\n",
      "5 3.745664739884393 1.254335260115607\n",
      "6 4.277777777777778 1.7222222222222223\n",
      "7 7.393939393939394 -0.3939393939393936\n",
      "8 6.049180327868853 1.9508196721311473\n",
      "9 7.804878048780488 1.1951219512195124\n",
      "10 8.25 1.75\n",
      "VALUE OF D OBTAINED IS = 0.947033\n"
     ]
    }
   ],
   "source": [
    "\"\"\" GOOD TURING SMOOTHING - FINDING VALUE OF D\"\"\"\n",
    "\n",
    "# CREATE FREQUENCY OF FREQUENCIES\n",
    "freqDict = dict()\n",
    "for k in bigramCount.values():\n",
    "    if(k not in freqDict):\n",
    "        freqDict[k] = 0\n",
    "    freqDict[k] += 1\n",
    "\n",
    "freqDict[0] = len(bigrams)\n",
    "newCount = {}\n",
    "for k in range(10, -1, -1):\n",
    "    newCount[k] = ((k + 1) * freqDict[k + 1]) / freqDict[k]\n",
    "\n",
    "for k in list(newCount.keys())[::-1]:\n",
    "    print(k, newCount[k], k - newCount[k])\n",
    "\n",
    "print(\"VALUE OF D OBTAINED IS = %f\" % ((1-newCount[1]+2-newCount[2]+3-newCount[3]+4-newCount[4]+5-newCount[5])/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "('And', 'now')\n",
      "MLE before Good Turing smoothing - 0.017857\n",
      "After Good Turing smoothing - 0.005265\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "('now', 'which')\n",
      "MLE before Good Turing smoothing - 0.029412\n",
      "After Good Turing smoothing - 0.008672\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "('which', 'is')\n",
      "MLE before Good Turing smoothing - 0.030303\n",
      "After Good Turing smoothing - 0.008935\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\" GOOD TURING SMOOTHING - FINDING MLEs \"\"\"\n",
    "\n",
    "bigramGoodTuringMLE = {}\n",
    "for key in bigramCount:\n",
    "    if(bigramCount[key] in newCount):\n",
    "        bigramGoodTuringMLE[key] = newCount[bigramCount[key]]/unigramCount[(key[0], )]\n",
    "    else:\n",
    "        bigramGoodTuringMLE[key] = bigramMLE[key]\n",
    "        \n",
    "for key in list(bigramMLE.keys())[1:4]:\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(key)\n",
    "    print(\"MLE before Good Turing smoothing - %f\" % bigramMLE[key])\n",
    "    print(\"After Good Turing smoothing - %f\" % bigramGoodTuringMLE[key])\n",
    "    print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity value for Add-1 smoothing - \n",
      "5819.094621780029\n"
     ]
    }
   ],
   "source": [
    "\"\"\" PERPLEXITY VALUE FOR TEST DATASET USING ADD-1 SMOOTHING \"\"\"\n",
    "\n",
    "def gen_bigrams_test():\n",
    "    ngrams_list = []\n",
    "    for sentence in sentences_list_test:\n",
    "        ngrams_list = ngrams_list + list(ngrams(sentence.split(\" \"), 2))\n",
    "    return ngrams_list\n",
    "bigramsTest = gen_bigrams_test()\n",
    "\n",
    "testSetSize = 0\n",
    "for sentence in sentences_list_test:\n",
    "    testSetSize += len(sentence.split(\" \"))\n",
    "\n",
    "perplexityAddOne = 1.0\n",
    "for k in bigramsTest:\n",
    "    if(k in bigramAddOneMLE):\n",
    "        perplexityAddOne *= ((1.0/bigramAddOneMLE[k]) ** (1.0/testSetSize))\n",
    "    else:\n",
    "        perplexityAddOne *= (len(unigrams) ** (1.0/testSetSize))\n",
    "\n",
    "print(\"Perplexity value for Add-1 smoothing - \")\n",
    "print(perplexityAddOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity value for Good Turing smoothing - \n",
      "719.0790952688778\n"
     ]
    }
   ],
   "source": [
    "\"\"\" PERPLEXITY VALUE FOR TEST DATASET USING GOOD TURING SMOOTHING \"\"\"\n",
    "\n",
    "perplexityGoodTuring = 1.0\n",
    "for k in bigramsTest:\n",
    "    if(k in bigramGoodTuringMLE):\n",
    "        perplexityGoodTuring *= ((1.0/bigramGoodTuringMLE[k]) ** (1.0/testSetSize))\n",
    "    else:\n",
    "        perplexityGoodTuring *= ((len(unigrams)/newCount[0]) ** (1.0/testSetSize))\n",
    "\n",
    "print(\"Perplexity value for Good Turing smoothing - \")\n",
    "print(perplexityGoodTuring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since, perplexity value for Good Turing smoothing is less than Add-1 smoothing, we can say that Good Turing smoothing performs better on the test set as compared to Add-1 smoothing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
